{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d6a9143-d007-4131-bd0c-06560486ff4a",
   "metadata": {},
   "source": [
    "# I. Big Data\n",
    "COllection of data sets so large and complex that it becomes difficult to process using on hand database system tools or traditional data processing Applications<br>\n",
    "Too much data to process<br>\n",
    "Huge in volume and growng exponentially with time<br>\n",
    "We need complex algorithms and mathematical and statistical methods to process this data such as Data visualization , Data Mining.<br>\n",
    "eg: Stock Exchange - 1TB data/day\n",
    "Jet Engine - 10 TB/30 mins\n",
    "Facebook-500+TB/day\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00b81de-0bac-48aa-9e9b-308117bf5ca9",
   "metadata": {},
   "source": [
    "# II. 5V's of Big Data\n",
    "\n",
    "### 1.Volume\n",
    "By 2020 accumulated digital universe of data wll grow from 44 Zettabytes or 44 trillion gigabytes<br>\n",
    "Data come from multiple sources in huge amounts\n",
    "\n",
    "### 2.Variety\n",
    "Data can be \n",
    "Tye\n",
    "#### Structured-\n",
    "Proper Schema<br>\n",
    "Any data that can be stored  accessed and processed in the form of a fixed format<br>\n",
    "Tables such as SQL  <br>\n",
    "\n",
    "#### Semi-structured-\n",
    "Mixed data- contains both structured and un structured data<br>\n",
    "JSON,XML,CSV <br>\n",
    "\n",
    "#### Unstructured -\n",
    "Data available in multiple formats , no specific format.<br>\n",
    "Any data with unknown form or structure<br>\n",
    "Logs , Audio , Video ,Image<br>\n",
    "\n",
    "\n",
    "### 3. Velocity\n",
    "Speed at which data is generated in real-time<br>\n",
    "Rate of data accumulation, change<br>\n",
    "Speed or accumulation of all these data together<br> \n",
    "Earlier using MainFrame systems-Huge computers but less data<br>\n",
    "Client Server model-Web applicatiions increased along with users who use from computers and mobile devices<br>\n",
    "Social Media - 100000 tweets/60 seconds , ststus , messages <br>\n",
    "Google searches <br>\n",
    "\n",
    "\n",
    "### 4. Value\n",
    "Extract useful data from these huge dataset<br>\n",
    "Can be achieved usig analyics<br>\n",
    "Make sure all these huge amount of data is definitely helping us in one or other way in our business goals\n",
    "\n",
    "\n",
    "### 5.Veracity\n",
    "Big data has lot of inconsistency aand uncertainity<br>\n",
    "some data packets are bound to lose in the process<br>\n",
    "Fill up missing data , perform mining , process it and come up with a good insight<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bc762b-e4a8-417d-96a4-59bb0affb2a4",
   "metadata": {},
   "source": [
    "# Applications\n",
    "HealthCare<br>\n",
    "Academia<br>\n",
    "Banking<br>\n",
    "Manufacturing<br>\n",
    "Ecommerce<br>\n",
    "IT<br>\n",
    "Transportation<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea8a580-f0a9-4979-9968-91d7c61b81cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# III. Big Data Life Cycle\n",
    "\n",
    "## 1.Business Evaluation\n",
    "Understand Project objectives and requirements from a business perspective , and then covnverting this knowledge into data mining problem definition<br>\n",
    "\n",
    "## 2.Data Understanding\n",
    "Starts with Initial data collection\n",
    "Activities to get familiar with the data\n",
    "Understand data quality problems\n",
    "First insights to data \n",
    "Perform Hypothesis for hidden information\n",
    "\n",
    "## 3.Data Preparation\n",
    "All activities to construct final dataset\n",
    "This is the data that is fed into modelling tools from initial raw data\n",
    "Data Integration , Transformation , Mapping\n",
    "\n",
    "## 4.Data Modelling\n",
    "Various modelling techniques are selected and applied and their parameters are calibrated to optimum values<br>\n",
    "\n",
    "\n",
    "## 5.Evaluation\n",
    "Check if the model we built is suitable for analytics and if it can satisfy our requirement<br>\n",
    "Different kinds of testing is done on the data to make sure itmeets customer requirement<br>\n",
    "Data Analytics perspective is considered and check if this model is suitable for analytics<br>\n",
    "\n",
    "## 6.Deployment\n",
    "The created model is organised and presented in a way that is useful to the customer<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8907a73a-859c-41d2-97e5-aaf2059e2007",
   "metadata": {
    "tags": []
   },
   "source": [
    "# IV. Data Cleaning\n",
    "The Layer called DataLake takes care of Data Cleaning\n",
    "\n",
    "Its about checking the data whether it corrupted or not\n",
    "Involves removal of unwanted data\n",
    "FInd missing values in the dataset and fill it with genuine values\n",
    "Check the original data source had any such issues or if its the data thats given to us has such issues\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e13f013-f49a-4d65-8fe4-de8c7f866aa8",
   "metadata": {},
   "source": [
    "## Why Data Cleaning\n",
    "\n",
    "### 1.Efficiency\n",
    "\n",
    "Having clean data (free from wrong and inconsistent values) can help you in performing your analysis a lot faster. <br>\n",
    "Save a considerable amount of time by doing this task beforehand. When you clean your data before using it, you’d be able to avoid multiple errors. <br>\n",
    "If you use data containing false values, your results won’t be accurate.<br>\n",
    "\n",
    "Redo the entire task again, which can cause a lot of waste of time. <br>\n",
    "If you choose to clean your data before using it, you can generate results faster and avoid redoing the entire task again. <br>\n",
    "\n",
    "\n",
    "### 2.Error Margin\n",
    "\n",
    "When you don’t use accurate data for analysis, you will surely make mistakes. <br>\n",
    "If data we present still has errors , not just wastage of time but they also waste resources.  <br>\n",
    "Data cleansing helps you in that regard full stop it is a widespread practice, and you should learn the methods used to clean data. <br>\n",
    "\n",
    "Using a simple algorithm with clean data is way better than using an advanced with unclean data.\n",
    "\n",
    "### 3.Determining Data Quality\n",
    "Is The Data Valid? (Validity) <br>\n",
    "\n",
    "The validity of your data is the degree to which it follows the rules of your particular requirements. <br>\n",
    "For example, you how to import phone numbers of different customers, but in some places, you added email addresses in the data.  <br>\n",
    "Now because your needs were explicitly for phone numbers, the email addresses would be invalid.  <br>\n",
    "\n",
    "Validity errors take place when the input method isn’t properly inspected.  <br>\n",
    "You might be using spreadsheets for collecting your data. And you might enter the wrong information in the cells of the spreadsheet.  <br>\n",
    "\n",
    "There are multiple kinds of constraints your data has to conform to for being valid. Here they are:\n",
    "\n",
    "#### a.Range: \n",
    "\n",
    "Some types of numbers have to be in a specific range. For example, the number of products you can transport in a day must have a minimum and maximum value. <br>\n",
    "There would surely be a particular range for the data. There would be a starting point and an end-point.  <br>\n",
    "\n",
    "#### b.Data-Type: \n",
    "\n",
    "Some data cells might require a specific kind of data, such as numeric, Boolean, etc. For example, in a Boolean section, you wouldn’t add a numerical value. <br>\n",
    "\n",
    "#### c.Compulsory constraints:\n",
    "\n",
    "In every scenario, there are some mandatory constraints your data should follow.  <br>\n",
    "The compulsory restrictions depend on your specific needs. Surely, specific columns of your data shouldn’t be empty.For example, in the list of your clients’ names, the column of ‘name’ can’t be empty. \n",
    "\n",
    "#### d.Cross-field examination:\n",
    "\n",
    "There are certain conditions which affect multiple fields of data in a particular form. Suppose the time of departure of a flight couldn’t be earlier than it’s arrival. <br>\n",
    "In a balance sheet, the sum of the debit and credit of the client must be the same. It can’t be different.  <br>\n",
    "\n",
    "These values are related to each other, and that’s why you might need to perform cross-field examination. \n",
    "\n",
    "#### e.Unique Requirements:\n",
    "\n",
    "Particulars types of data have unique restrictions. Two customers can’t have the same customer support ticket. <br>\n",
    "Such kind of data must be unique to a particular field and can’t be shared by multiple ones.  <br>\n",
    "\n",
    "#### f.Set-Membership Restrictions:\n",
    "\n",
    "Some values are restricted to a particular set. Like, gender can either be Male, Female or Unknown.  <br>\n",
    "\n",
    "#### g.Regular Patterns:\n",
    "\n",
    "Some pieces of data follow a specific format. For example, email addresses have the format ‘randomperson@randomemail.com’. Similarly, phone numbers have ten digits. <br>\n",
    "\n",
    "If the data isn’t in the required format, it would also be invalid.  <br>\n",
    "\n",
    "If a person omits the ‘@’ while entering an email address, then the email address would be invalid. <br>\n",
    "Checking the validity of your data is the first step to determine its quality. Most of the time, the cause of entry of invalid information is human error. <br>\n",
    "Getting rid of it will help you in streamlining your process and avoiding useless data values beforehand.  <br>\n",
    "\n",
    "### 4.Accuracy\n",
    "\n",
    "Most of the data you have is valid, you’ll have to focus on establishing its accuracy. Even though the data is valid, it doesn’t mean the data is accurate. <br>\n",
    "And determining accuracy helps you to figure out if the data you entered was accurate or not.  <br>\n",
    "\n",
    "The address of a client could be in the right format, but it doesn’t need to be the right one. Maybe the email has an additional digit or character that makes it wrong. \n",
    "Another example is of the phone number of a customer.  <br>\n",
    "\n",
    "\n",
    "If the phone number has all the digits, it’s a valid value. But that doesn’t mean it’s true. <br>\n",
    "\n",
    "When you have definitions for valid values, figuring out the invalid ones is easy. But that doesn’t help with checking the accuracy of the same. Checking the accuracy of your data values requires you to use third-party sources.  <br>\n",
    "\n",
    "This means you’ll have to rely on data sources different from the one you’re using currently. <br>\n",
    "You’ll have to cross-check your data to figure out if it’s accurate or not.  <br>\n",
    "Data cleaning techniques don’t have many solutions for checking the accuracy of data values.  <br>\n",
    "\n",
    "However, depending on the kind of data you’re using, you might be able to find resources that could help you in this regard. You shouldn’t confuse accuracy with precision. <br>\n",
    "\n",
    "#### a.Accuracy vs Precision\n",
    "\n",
    "While accuracy relies on establishing whether your entered data was correct or not, precision requires you to give more details about the same. <br>\n",
    "A customer might enter a first name in your data field. But if there’s no last name, it’d be challenging to be more precise. <br>\n",
    "\n",
    "Another example can be of an address. Suppose you ask a person where he/she lives. They might say that they live in London. That could be true. However, that’s not a precise answer because you don’t know where they live in London. <br>\n",
    "\n",
    "A precise answer would be to give you a street address. \n",
    "\n",
    "### 5.Completeness\n",
    "\n",
    "It’s nearly impossible to have all the info you need. Completeness is the degree to which you know all the required values.  <br>\n",
    "Completeness is a little more challenging to achieve than accuracy or validity. That’s because we can’t assume a value. You only have to enter known facts. <br>\n",
    "\n",
    "You can try to complete your data by redoing the data gathering activities (approaching the clients again, re-interviewing people, etc.). <br>\n",
    "But that doesn’t mean you’d be able to complete your data thoroughly.  <br>\n",
    "\n",
    "Suppose you re-interview people for the data you needed earlier. Now, this scenario has the problem of recall. <br>\n",
    "If you ask them the same questions again, chances are, they might not remember what they had answered before. This can lead to them, giving you the wrong answer.  <br>\n",
    "\n",
    "You might ask him what books they were reading five months ago. And they might not remember. Similarly, you might need to enter every customer’s contact information. But some of them may not have email addresses. In this case, you’d have to leave those columns empty. <br>\n",
    "\n",
    "If you have a system which requires you to fill all columns, you can try to enter ‘missing’ or ‘unknown’ there. But entering such values doesn’t mean the data is complete. It would be still be referred to as incomplete.  <br>\n",
    "\n",
    "### 6.Consistency\n",
    "\n",
    "Next to completeness comes consistency. You can measure consistency by comparing two similar systems. Or, you can check the data values within the same dataset to see if they are consistent or not. <br>\n",
    "Consistency can be relational. For example, a customer’s age might be 15, which is a valid value and could be accurate, but they might also be stated senior-citizen in the same system. <br>\n",
    "\n",
    "In such cases, you’ll need to cross-check the data, similar to measuring accuracy, and see which value is true. Is the client a 15-year old? Or is the client a senior-citizen? Only one of these values could be true. <br>\n",
    "\n",
    "There are multiple ways to make your data consistent.\n",
    "\n",
    "#### a.Check different systems:\n",
    "\n",
    "You can take a look at another similar system to find whether the value you have is real or not. If two of your systems are contradicting each other, it might help to check the third one.  <br>\n",
    "\n",
    "In our previous example, suppose you check the third system and find the age of the customer is 65. This shows that the second system, which said the customer is a senior citizen, would hold. <br>\n",
    "\n",
    "#### b.Check the latest data:\n",
    "\n",
    "Another way to improve the consistency of your data is to check the more recent value. It can be more beneficial to you in specific scenarios. You might have two different contact numbers for a customer in your record. The most recent one would probably be more reliable because it’s possible that the customer switched numbers. \n",
    "\n",
    "#### c.Check the source:\n",
    "\n",
    "The most fool-proof way to check the reliability of the data is to contact the source simply. In our example of the customer’s age, you can opt to contact the customer directly and ask them their age. <br>\n",
    "However, it’s not possible in every scenario and directly contacting the source can be highly tricky. Maybe the customer doesn’t respond, or their contact information isn’t available.  <br>\n",
    "\n",
    "### 7.Uniformity\n",
    "\n",
    "You should ensure that all the values you’ve entered in your dataset are in the same units. If you’re entering SI units for measurements, you can’t use the Imperial system in some places. On the other hand, if at one place you’ve entered the time in seconds, then you should enter it in this format all across the dataset. <br>\n",
    "\n",
    "\n",
    "Checking the uniformity of your records is quite easy. A simple inspection can reveal whether a particular value is in the required unit or not. The units you use for entering your data depend on your specific requirements. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95766313-fed5-4825-9743-06c0cc775f08",
   "metadata": {},
   "source": [
    "# Data Cleansing Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e9d4b6-1394-4ff0-859c-ef191c0000e7",
   "metadata": {},
   "source": [
    "Your choice of data cleaning techniques relies on what kind of data are you dealing with? <br>\n",
    "Are they numeric values or strings?<br>\n",
    "Unless you have too few values to handle, you shouldn’t expect to clean your data with just one technique as well.<br>\n",
    "\n",
    "You might need to use multiple techniques for a better result. The more data types you have to handle, the more cleansing techniques you’ll have to use. <br>\n",
    "Being familiar with all of these methods will help you in rectifying errors and getting rid of useless data. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2156b552-d14f-41e9-8528-0b906c80cdab",
   "metadata": {},
   "source": [
    "## 1. Remove Irrelevant Values\n",
    "\n",
    "The first and foremost thing you should do is remove useless pieces of data from your system. <br>\n",
    "Any useless or irrelevant data is the one you don’t need. It might not fit the context of your issue. <br>\n",
    "\n",
    "Eg: You might only have to measure the average age of your sales staff. Then their email address wouldn’t be required.<br>\n",
    "Another example is you might be checking to see how many customers you contacted in a month. In this case, you wouldn’t need the data of people you reached in a prior month.<br>\n",
    "\n",
    "Before you remove a particular piece of data, make sure that it is irrelevant because you might need it to check its correlated values later on (for checking the consistency).<br>\n",
    "And if you can get a second opinion from a more experienced expert before removing data, feel free to do so. <br>\n",
    "\n",
    "Dont delete some values and regret the decision later on. But once you’re assured that the data is irrelevant, get rid of it. <br>\n",
    "\n",
    "## 2.Get Rid of Duplicate Values\n",
    "\n",
    "Duplicates are similar to useless values – You don’t need them. <br>\n",
    "They only increase the amount of data you have and waste your time. <br>\n",
    "You can get rid of them with simple searches. Duplicate values could be present in your system for several reasons.<br>\n",
    "\n",
    "Maybe you combined the data of multiple sources. Or, perhaps the person submitting the data repeated a value mistakingly.<br>\n",
    "Some user clicked twice on ‘enter’ when they were filling an online form. You should remove the duplicates as soon as you find them. <br>\n",
    "\n",
    "## 3. Avoid Typos (and similar errors)\n",
    "\n",
    "Typos are a result of human error and can be present anywhere.<br> You can fix typos through multiple algorithms and techniques. <br>\n",
    "You can map the values and convert them into the correct spelling. <br>\n",
    "Typos are essential to fix because models treat different values differently.<br>\n",
    "Strings rely a lot on their spellings and cases<br>\n",
    "\n",
    "‘George’ is different from ‘george’ even though they have the same spelling.<br>\n",
    "Similarly ‘Mike’ and ‘Mice’ are different from each other, also though they have the same number of characters.<br>\n",
    "You’ll need to look for typos such as this and fix them appropriately. <br>\n",
    "\n",
    "Another error similar to typos is of strings’ size.<br>\n",
    "You might need to pad them to keep them in the same format.<br>\n",
    "For example, your dataset might require you to have 5-digit numbers only. So if you have any value which only has four digits such as ‘3994’ you can add a zero in the beginning to increase its number of digits.<br>\n",
    "\n",
    "Its value would remain the same as ‘03994’, but it’ll keep your data uniform.<br>\n",
    "An additional error with strings is of white spaces.\n",
    "Make sure you remove them from your strings to keep them consistent. <br>\n",
    "\n",
    "## 4.Convert Data Types\n",
    "\n",
    "Data types should be uniform across your dataset. <br>A string can’t be numeric nor can a numeric be a boolean.<br> There are several things you should keep in mind when it comes to converting data types:\n",
    "\n",
    "    Keep numeric values as numerics\n",
    "    Check whether a numeric is a string or not. If you entered it as a string, it would be incorrect. \n",
    "    If you can’t convert a specific data value, you should enter ‘NA value’ or something of this sort. \n",
    "Make sure you add a warning as well to show that this particular value is wrong.<br>\n",
    "\n",
    "## 5.Take Care of Missing Values\n",
    "\n",
    "There would always be a piece of missing data. You can’t avoid it.<br>\n",
    "So you should know how to handle them to keep your data clean and free from errors. <br>\n",
    "A particular column in your dataset may have too many missing values ,In that case, it would be wise to get rid of the entire column because it doesn’t have enough data to work with.\n",
    "\n",
    "Point to note: You shouldn’t ignore missing values.<br>\n",
    "\n",
    "Ignoring missing values can be a significant mistake because they will contaminate your data, and you won’t get accurate results.<br>\n",
    "There are multiple ways to deal with missing values. <br>\n",
    "\n",
    "### a.Imputing Missing Values:\n",
    "\n",
    "You can impute missing values, which means, assuming the approximate value.<br>\n",
    "You can use linear regression or median to calculate the missing value.<br>\n",
    "This method has its implications because you can’t be sure if that would be the real value. \n",
    "\n",
    "Another method to impute missing values is to copy the data from a similar dataset.<br>\n",
    "This method is called ‘Hot-deck imputation’.<br>\n",
    "You’re adding value in your current record while considering some constraints such as data-type and range. <br>\n",
    "\n",
    "### b.Highlighting Missing Values:\n",
    "\n",
    "Imputation isn’t always the best measure to take care of missing values.<br>\n",
    "Many experts argue that it only leads to more mixed results as they are not ‘real’.<br>\n",
    "So, you can take another approach and inform the model that the data is missing. Telling the model (or the algorithm) that the specific value is unavailable can be a piece of information as well. \n",
    "\n",
    "If random reasons aren’t responsible for your missing values, it can be beneficial to highlight or flag them.<br>\n",
    "For example, your records may not have many answers to a specific question of your survey because your customer didn’t want to answer it in the first place. <br>\n",
    "\n",
    "If the missing value is numeric, you can use 0 but make sure that you ignore these values during statistical analysis.<br>\n",
    "On the other hand, if the missing value is a categorical value, you can fill ‘missing’<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290060e1-7482-47a9-8031-c47a04b1853a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
